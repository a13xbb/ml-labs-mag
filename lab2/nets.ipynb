{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alex/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/alex/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import MLP, LSTM\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('max_colwidth', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw_df = pd.read_csv('data/train.csv')\n",
    "test_raw_df = pd.read_csv('data/test.csv')\n",
    "test_raw_df = test_raw_df.drop(columns=['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reuters)</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new menace to US economy (AFP)</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>Pakistan's Musharraf Says Won't Quit as Army Chief</td>\n",
       "      <td>KARACHI (Reuters) - Pakistani President Pervez Musharraf  has said he will stay on as army chief, reneging on a pledge to  quit the powerful post by the end of the year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>Renteria signing a top-shelf deal</td>\n",
       "      <td>Red Sox general manager Theo Epstein acknowledged Edgar Renteria was more a luxury for the 2005 Red Sox than a necessity. But there's nothing wrong with getting the keys to a BMW, and that's what the four-time All-Star and two-time Gold Glover is in the eyes of the Red Sox.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>Saban not going to Dolphins yet</td>\n",
       "      <td>The Miami Dolphins will put their courtship of LSU coach Nick Saban on hold to comply with the NFL's hiring policy by interviewing at least one minority candidate, a team source told The Associated Press last night.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>Today's NFL games</td>\n",
       "      <td>PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: Steelers by 10. Records: Steelers 12-1, Giants 5-8. Vs. spread: Steelers 10-1-2, Giants 5-8. Series: Giants lead, 43-27-3. Comments: Think the Giants knew Ben Roethlisberger was available on draft day when they broke the bank and traded for Eli Manning? . . . All Big Ben has done this year is complete ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>Nets get Carter from Raptors</td>\n",
       "      <td>INDIANAPOLIS -- All-Star Vince Carter was traded by the Toronto Raptors to the New Jersey Nets for Alonzo Mourning, Eric Williams, Aaron Williams, and a pair of first-round draft picks yesterday.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class Index  \\\n",
       "0       3             \n",
       "1       3             \n",
       "2       3             \n",
       "3       3             \n",
       "4       3             \n",
       "...    ..             \n",
       "119995  1             \n",
       "119996  2             \n",
       "119997  2             \n",
       "119998  2             \n",
       "119999  2             \n",
       "\n",
       "                                                                            Title  \\\n",
       "0       Wall St. Bears Claw Back Into the Black (Reuters)                           \n",
       "1       Carlyle Looks Toward Commercial Aerospace (Reuters)                         \n",
       "2       Oil and Economy Cloud Stocks' Outlook (Reuters)                             \n",
       "3       Iraq Halts Oil Exports from Main Southern Pipeline (Reuters)                \n",
       "4       Oil prices soar to all-time record, posing new menace to US economy (AFP)   \n",
       "...                                                                           ...   \n",
       "119995  Pakistan's Musharraf Says Won't Quit as Army Chief                          \n",
       "119996  Renteria signing a top-shelf deal                                           \n",
       "119997  Saban not going to Dolphins yet                                             \n",
       "119998  Today's NFL games                                                           \n",
       "119999  Nets get Carter from Raptors                                                \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                             Description  \n",
       "0       Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.                                                                                                                                                                                                                                                                    \n",
       "1       Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.                                                                                                                                            \n",
       "2       Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.                                                                                                                                                                          \n",
       "3       Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.                                                                                                                                                               \n",
       "4       AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.                                                                                                                                                                                                  \n",
       "...                                                                                                                                                                  ...                                                                                                                                                                                                  \n",
       "119995   KARACHI (Reuters) - Pakistani President Pervez Musharraf  has said he will stay on as army chief, reneging on a pledge to  quit the powerful post by the end of the year.                                                                                                                                                                                        \n",
       "119996  Red Sox general manager Theo Epstein acknowledged Edgar Renteria was more a luxury for the 2005 Red Sox than a necessity. But there's nothing wrong with getting the keys to a BMW, and that's what the four-time All-Star and two-time Gold Glover is in the eyes of the Red Sox.                                                                                \n",
       "119997  The Miami Dolphins will put their courtship of LSU coach Nick Saban on hold to comply with the NFL's hiring policy by interviewing at least one minority candidate, a team source told The Associated Press last night.                                                                                                                                           \n",
       "119998  PITTSBURGH at NY GIANTS Time: 1:30 p.m. Line: Steelers by 10. Records: Steelers 12-1, Giants 5-8. Vs. spread: Steelers 10-1-2, Giants 5-8. Series: Giants lead, 43-27-3. Comments: Think the Giants knew Ben Roethlisberger was available on draft day when they broke the bank and traded for Eli Manning? . . . All Big Ben has done this year is complete ...  \n",
       "119999  INDIANAPOLIS -- All-Star Vince Carter was traded by the Toronto Raptors to the New Jersey Nets for Alonzo Mourning, Eric Williams, Aaron Williams, and a pair of first-round draft picks yesterday.                                                                                                                                                               \n",
       "\n",
       "[120000 rows x 3 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fears for T N pension after talks</td>\n",
       "      <td>Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com)</td>\n",
       "      <td>SPACE.com - TORONTO, Canada -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ky. Company Wins Grant to Study Peptides (AP)</td>\n",
       "      <td>AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prediction Unit Helps Forecast Wildfires (AP)</td>\n",
       "      <td>AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Calif. Aims to Limit Farm-Related Smog (AP)</td>\n",
       "      <td>AP - Southern California's smog-fighting agency went after emissions of the bovine variety Friday, adopting the nation's first rules to reduce air pollution from dairy cow manure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>Around the world</td>\n",
       "      <td>Ukrainian presidential candidate Viktor Yushchenko was poisoned with the most harmful known dioxin, which is contained in Agent Orange, a scientist who analyzed his blood said Friday.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>Void is filled with Clement</td>\n",
       "      <td>With the supply of attractive pitching options dwindling daily -- they lost Pedro Martinez to the Mets, missed on Tim Hudson, and are resigned to Randy Johnson becoming a Yankee -- the Red Sox struck again last night, coming to terms with free agent Matt Clement on a three-year deal that will pay the righthander in the neighborhood of \\$25 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>Martinez leaves bitter</td>\n",
       "      <td>Like Roger Clemens did almost exactly eight years earlier, Pedro Martinez has left the Red Sox apparently bitter about the way he was treated by management.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>5 of arthritis patients in Singapore take Bextra or Celebrex &amp;lt;b&amp;gt;...&amp;lt;/b&amp;gt;</td>\n",
       "      <td>SINGAPORE : Doctors in the United States have warned that painkillers Bextra and Celebrex may be linked to major cardiovascular problems and should not be prescribed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>EBay gets into rentals</td>\n",
       "      <td>EBay plans to buy the apartment and home rental service Rent.com for \\$415 million, adding to its already exhaustive breadth of offerings.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                       Title  \\\n",
       "0     Fears for T N pension after talks                                                        \n",
       "1     The Race is On: Second Private Team Sets Launch Date for Human Spaceflight (SPACE.com)   \n",
       "2     Ky. Company Wins Grant to Study Peptides (AP)                                            \n",
       "3     Prediction Unit Helps Forecast Wildfires (AP)                                            \n",
       "4     Calif. Aims to Limit Farm-Related Smog (AP)                                              \n",
       "...                                           ...                                              \n",
       "7595  Around the world                                                                         \n",
       "7596  Void is filled with Clement                                                              \n",
       "7597  Martinez leaves bitter                                                                   \n",
       "7598  5 of arthritis patients in Singapore take Bextra or Celebrex &lt;b&gt;...&lt;/b&gt;      \n",
       "7599  EBay gets into rentals                                                                   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                    Description  \n",
       "0     Unions representing workers at Turner   Newall say they are 'disappointed' after talks with stricken parent firm Federal Mogul.                                                                                                                                                                                                                            \n",
       "1     SPACE.com - TORONTO, Canada -- A second\\team of rocketeers competing for the  #36;10 million Ansari X Prize, a contest for\\privately funded suborbital space flight, has officially announced the first\\launch date for its manned rocket.                                                                                                                 \n",
       "2     AP - A company founded by a chemistry researcher at the University of Louisville won a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins.                                                                                                                                     \n",
       "3     AP - It's barely dawn when Mike Fitzpatrick starts his shift with a blur of colorful maps, figures and endless charts, but already he knows what the day will bring. Lightning will strike in places he expects. Winds will pick up, moist places will dry and flames will roar.                                                                           \n",
       "4     AP - Southern California's smog-fighting agency went after emissions of the bovine variety Friday, adopting the nation's first rules to reduce air pollution from dairy cow manure.                                                                                                                                                                        \n",
       "...                                                                                                                                                                                   ...                                                                                                                                                                        \n",
       "7595  Ukrainian presidential candidate Viktor Yushchenko was poisoned with the most harmful known dioxin, which is contained in Agent Orange, a scientist who analyzed his blood said Friday.                                                                                                                                                                    \n",
       "7596  With the supply of attractive pitching options dwindling daily -- they lost Pedro Martinez to the Mets, missed on Tim Hudson, and are resigned to Randy Johnson becoming a Yankee -- the Red Sox struck again last night, coming to terms with free agent Matt Clement on a three-year deal that will pay the righthander in the neighborhood of \\$25 ...  \n",
       "7597  Like Roger Clemens did almost exactly eight years earlier, Pedro Martinez has left the Red Sox apparently bitter about the way he was treated by management.                                                                                                                                                                                               \n",
       "7598  SINGAPORE : Doctors in the United States have warned that painkillers Bextra and Celebrex may be linked to major cardiovascular problems and should not be prescribed.                                                                                                                                                                                     \n",
       "7599  EBay plans to buy the apartment and home rental service Rent.com for \\$415 million, adding to its already exhaustive breadth of offerings.                                                                                                                                                                                                                 \n",
       "\n",
       "[7600 rows x 2 columns]"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_raw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning data\n",
    "\n",
    "Очевидно, что удаление знаков препинания и других символов необходимо, чтобы избежать кодирования не несущих смысл символов, сделаем это для всех экспериментов. Затем сравним удаление стоп-слов, удаление цифр и их комбирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls count:\n",
      "Class Index    0\n",
      "Title          0\n",
      "Description    0\n",
      "dtype: int64\n",
      "\n",
      "Empty string count:\n",
      "Class Index    0\n",
      "Title          0\n",
      "Description    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Nulls count:\", train_raw_df.isna().sum(), sep='\\n')\n",
    "print()\n",
    "print(\"Empty string count:\", train_raw_df.eq('').sum(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls count:\n",
      "Title          0\n",
      "Description    0\n",
      "dtype: int64\n",
      "\n",
      "Empty string count:\n",
      "Title          0\n",
      "Description    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Nulls count:\", test_raw_df.isna().sum(), sep='\\n')\n",
    "print()\n",
    "print(\"Empty string count:\", test_raw_df.eq('').sum(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(train_df, remove_digits=False, remove_stop_words=False):\n",
    "    train_df = train_df.copy(deep=True)\n",
    "\n",
    "    # train_df['text'] = train_raw_df['Title'] + \" \" + train_raw_df['Description']\n",
    "    train_df['text'] = train_df['Description']\n",
    "    train_df.drop(columns=['Title', 'Description'], inplace=True)\n",
    "\n",
    "    train_df['text'] = train_df['text'].str.replace(\"\\\\\", \" \").str.lower()\n",
    "    \n",
    "    if remove_digits:\n",
    "        train_df['text'] = train_df['text'].str.replace(r'\\d', ' ', regex=True)\n",
    "\n",
    "    if remove_stop_words:\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words_pattern = r'\\b(?:' + '|'.join(re.escape(word) for word in stop_words) + r')\\b'\n",
    "        train_df['text'] = train_df['text'].str.replace(stop_words_pattern, ' ', regex=True)\n",
    "\n",
    "    train_df['text'] = train_df['text'].str.replace(r'[^\\d\\w\\s]', '', regex=True)\n",
    "    train_df['text'] = train_df['text'].apply(word_tokenize, preserve_line=True)\n",
    "\n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_removed_all = clean_text(train_raw_df, remove_digits=True, remove_stop_words=True)\n",
    "test_removed_all = clean_text(test_raw_df, remove_digits=True, remove_stop_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class Index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>[reuters, shortsellers, wall, street, dwindling, band, ultracynics, seeing, green]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>[reuters, private, investment, firm, carlyle, group, reputation, making, welltimed, occasionally, controversial, plays, defense, industry, quietly, placed, bets, another, part, market]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[reuters, soaring, crude, prices, plus, worries, economy, outlook, earnings, expected, hang, stock, market, next, week, depth, summer, doldrums]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[reuters, authorities, halted, oil, export, flows, main, pipeline, southern, iraq, intelligence, showed, rebel, militia, could, strike, infrastructure, oil, official, said, saturday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>[afp, tearaway, world, oil, prices, toppling, records, straining, wallets, present, new, economic, menace, barely, three, months, us, presidential, elections]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119995</th>\n",
       "      <td>1</td>\n",
       "      <td>[karachi, reuters, pakistani, president, pervez, musharraf, said, stay, army, chief, reneging, pledge, quit, powerful, post, end, year]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119996</th>\n",
       "      <td>2</td>\n",
       "      <td>[red, sox, general, manager, theo, epstein, acknowledged, edgar, renteria, luxury, red, sox, necessity, nothing, wrong, getting, keys, bmw, fourtime, star, twotime, gold, glover, eyes, red, sox]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119997</th>\n",
       "      <td>2</td>\n",
       "      <td>[miami, dolphins, put, courtship, lsu, coach, nick, saban, hold, comply, nfl, hiring, policy, interviewing, least, one, minority, candidate, team, source, told, associated, press, last, night]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119998</th>\n",
       "      <td>2</td>\n",
       "      <td>[pittsburgh, ny, giants, time, p, line, steelers, records, steelers, giants, vs, spread, steelers, giants, series, giants, lead, comments, think, giants, knew, ben, roethlisberger, available, draft, day, broke, bank, traded, eli, manning, big, ben, done, year, complete]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119999</th>\n",
       "      <td>2</td>\n",
       "      <td>[indianapolis, star, vince, carter, traded, toronto, raptors, new, jersey, nets, alonzo, mourning, eric, williams, aaron, williams, pair, firstround, draft, picks, yesterday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class Index  \\\n",
       "0       3             \n",
       "1       3             \n",
       "2       3             \n",
       "3       3             \n",
       "4       3             \n",
       "...    ..             \n",
       "119995  1             \n",
       "119996  2             \n",
       "119997  2             \n",
       "119998  2             \n",
       "119999  2             \n",
       "\n",
       "                                                                                                                                                                                                                                                                                  text  \n",
       "0       [reuters, shortsellers, wall, street, dwindling, band, ultracynics, seeing, green]                                                                                                                                                                                              \n",
       "1       [reuters, private, investment, firm, carlyle, group, reputation, making, welltimed, occasionally, controversial, plays, defense, industry, quietly, placed, bets, another, part, market]                                                                                        \n",
       "2       [reuters, soaring, crude, prices, plus, worries, economy, outlook, earnings, expected, hang, stock, market, next, week, depth, summer, doldrums]                                                                                                                                \n",
       "3       [reuters, authorities, halted, oil, export, flows, main, pipeline, southern, iraq, intelligence, showed, rebel, militia, could, strike, infrastructure, oil, official, said, saturday]                                                                                          \n",
       "4       [afp, tearaway, world, oil, prices, toppling, records, straining, wallets, present, new, economic, menace, barely, three, months, us, presidential, elections]                                                                                                                  \n",
       "...                                                                                                                                                                ...                                                                                                                  \n",
       "119995  [karachi, reuters, pakistani, president, pervez, musharraf, said, stay, army, chief, reneging, pledge, quit, powerful, post, end, year]                                                                                                                                         \n",
       "119996  [red, sox, general, manager, theo, epstein, acknowledged, edgar, renteria, luxury, red, sox, necessity, nothing, wrong, getting, keys, bmw, fourtime, star, twotime, gold, glover, eyes, red, sox]                                                                              \n",
       "119997  [miami, dolphins, put, courtship, lsu, coach, nick, saban, hold, comply, nfl, hiring, policy, interviewing, least, one, minority, candidate, team, source, told, associated, press, last, night]                                                                                \n",
       "119998  [pittsburgh, ny, giants, time, p, line, steelers, records, steelers, giants, vs, spread, steelers, giants, series, giants, lead, comments, think, giants, knew, ben, roethlisberger, available, draft, day, broke, bank, traded, eli, manning, big, ben, done, year, complete]  \n",
       "119999  [indianapolis, star, vince, carter, traded, toronto, raptors, new, jersey, nets, alonzo, mourning, eric, williams, aaron, williams, pair, firstround, draft, picks, yesterday]                                                                                                  \n",
       "\n",
       "[120000 rows x 2 columns]"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_removed_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[unions, representing, workers, turner, newall, say, disappointed, talks, stricken, parent, firm, federal, mogul]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[spacecom, toronto, canada, second, team, rocketeers, competing, million, ansari, x, prize, contest, privately, funded, suborbital, space, flight, officially, announced, first, launch, date, manned, rocket]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ap, company, founded, chemistry, researcher, university, louisville, grant, develop, method, producing, better, peptides, short, chains, amino, acids, building, blocks, proteins]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ap, barely, dawn, mike, fitzpatrick, starts, shift, blur, colorful, maps, figures, endless, charts, already, knows, day, bring, lightning, strike, places, expects, winds, pick, moist, places, dry, flames, roar]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ap, southern, california, smogfighting, agency, went, emissions, bovine, variety, friday, adopting, nation, first, rules, reduce, air, pollution, dairy, cow, manure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>[ukrainian, presidential, candidate, viktor, yushchenko, poisoned, harmful, known, dioxin, contained, agent, orange, scientist, analyzed, blood, said, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>[supply, attractive, pitching, options, dwindling, daily, lost, pedro, martinez, mets, missed, tim, hudson, resigned, randy, johnson, becoming, yankee, red, sox, struck, last, night, coming, terms, free, agent, matt, clement, threeyear, deal, pay, righthander, neighborhood]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>[like, roger, clemens, almost, exactly, eight, years, earlier, pedro, martinez, left, red, sox, apparently, bitter, way, treated, management]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>[singapore, doctors, united, states, warned, painkillers, bextra, celebrex, may, linked, major, cardiovascular, problems, prescribed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>[ebay, plans, buy, apartment, home, rental, service, rentcom, million, adding, already, exhaustive, breadth, offerings]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                    text\n",
       "0     [unions, representing, workers, turner, newall, say, disappointed, talks, stricken, parent, firm, federal, mogul]                                                                                                                                                                 \n",
       "1     [spacecom, toronto, canada, second, team, rocketeers, competing, million, ansari, x, prize, contest, privately, funded, suborbital, space, flight, officially, announced, first, launch, date, manned, rocket]                                                                    \n",
       "2     [ap, company, founded, chemistry, researcher, university, louisville, grant, develop, method, producing, better, peptides, short, chains, amino, acids, building, blocks, proteins]                                                                                               \n",
       "3     [ap, barely, dawn, mike, fitzpatrick, starts, shift, blur, colorful, maps, figures, endless, charts, already, knows, day, bring, lightning, strike, places, expects, winds, pick, moist, places, dry, flames, roar]                                                               \n",
       "4     [ap, southern, california, smogfighting, agency, went, emissions, bovine, variety, friday, adopting, nation, first, rules, reduce, air, pollution, dairy, cow, manure]                                                                                                            \n",
       "...                                                                                                                                                                      ...                                                                                                            \n",
       "7595  [ukrainian, presidential, candidate, viktor, yushchenko, poisoned, harmful, known, dioxin, contained, agent, orange, scientist, analyzed, blood, said, friday]                                                                                                                    \n",
       "7596  [supply, attractive, pitching, options, dwindling, daily, lost, pedro, martinez, mets, missed, tim, hudson, resigned, randy, johnson, becoming, yankee, red, sox, struck, last, night, coming, terms, free, agent, matt, clement, threeyear, deal, pay, righthander, neighborhood]\n",
       "7597  [like, roger, clemens, almost, exactly, eight, years, earlier, pedro, martinez, left, red, sox, apparently, bitter, way, treated, management]                                                                                                                                     \n",
       "7598  [singapore, doctors, united, states, warned, painkillers, bextra, celebrex, may, linked, major, cardiovascular, problems, prescribed]                                                                                                                                             \n",
       "7599  [ebay, plans, buy, apartment, home, rental, service, rentcom, million, adding, already, exhaustive, breadth, offerings]                                                                                                                                                           \n",
       "\n",
       "[7600 rows x 1 columns]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_removed_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nulls count:\n",
      "\tRemoved both: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Nulls count:\")\n",
    "print(\"\\tRemoved both:\", train_removed_all['text'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizing(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join(lemmatizer.lemmatize(token) for token in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "    def __init__(self, vect_type, params = None) -> None:\n",
    "        self.vect_type = vect_type\n",
    "        self.params = params\n",
    "    \n",
    "    def _split_text(self, text):\n",
    "        return [sentence.split() for sentence in text]\n",
    "    \n",
    "    def fit(self, text):\n",
    "        if self.vect_type == \"word2vec\":\n",
    "            self.vectorizer = Word2Vec(sentences=self._split_text(text), **self.params)\n",
    "            \n",
    "        elif self.vect_type == \"fasttext\":\n",
    "            self.vectorizer = FastText(sentences=self._split_text(text), **self.params)\n",
    "            \n",
    "        elif self.vect_type == \"tfidf\":\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)\n",
    "            self.vectorizer.fit(text)\n",
    "            \n",
    "    def transform(self, text):\n",
    "        if self.vect_type == \"tfidf\":\n",
    "            return self.vectorizer.transform(text).toarray()\n",
    "        else:\n",
    "            embeddings = []\n",
    "            vector = self.vectorizer.wv\n",
    "            null_vect = np.zeros(self.params['vector_size'])\n",
    "\n",
    "            for sentence in self._split_text(text):\n",
    "                vectors = [vector[token] if token in vector else null_vect for token in sentence]\n",
    "                embeddings.append(np.mean(vectors, axis=0))\n",
    "            \n",
    "            return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделяем на трейн и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_removed_all\n",
    "X = data[['text']].copy()\n",
    "y = data[['Class Index']].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_removed_all\n",
    "X_test_subm = test_data[['text']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['text'] = X_train['text'].apply(lemmatizing)\n",
    "X_test['text'] = X_test['text'].apply(lemmatizing)\n",
    "\n",
    "X_test_subm['text'] = X_test_subm['text'].apply(lemmatizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизуем предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'vector_size': 100,\n",
    "            'window': 5,\n",
    "            'min_count': 4,\n",
    "            'workers': 4,\n",
    "         }\n",
    "\n",
    "vectorizer = Vectorizer(\"word2vec\", params)\n",
    "\n",
    "vectorizer.fit(X_train['text'])\n",
    "            \n",
    "X_train_vectorized = vectorizer.transform(X_train['text'])\n",
    "X_test_vectorized = vectorizer.transform(X_test['text'])\n",
    "\n",
    "X_test_subm_vectorized = vectorizer.transform(X_test_subm['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X: np.array, y: np.array=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if self.y is not None:\n",
    "            y = self.y[idx]\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        if self.y is not None:\n",
    "            return x, y\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ = np.array(y_train['Class Index']) - 1\n",
    "y_test_ = np.array(y_test['Class Index']) - 1\n",
    "\n",
    "train_dataset = NewsDataset(X_train_vectorized, y_train_)\n",
    "test_dataset = NewsDataset(X_test_vectorized, y_test_)\n",
    "\n",
    "test_dataset_no_gt = NewsDataset(X_test_vectorized)\n",
    "\n",
    "# train_size = int(0.8 * len(train_dataset))\n",
    "# val_size = len(train_dataset) - train_size\n",
    "\n",
    "# Разделение датасета\n",
    "# train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "# val_loader = Dataset(val_dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "test_loader_no_gt = DataLoader(test_dataset_no_gt, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subm_dataset = NewsDataset(X_test_subm_vectorized)\n",
    "test_subm_loader = DataLoader(test_subm_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_x in dataloader:\n",
    "            batch_x = batch_x.float()\n",
    "            outputs = model(batch_x)\n",
    "            logits = F.softmax(outputs)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            \n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            batch_x, batch_y = batch_x.float(), batch_y.long()\n",
    "            outputs = model(batch_x)\n",
    "            logits = F.softmax(outputs)\n",
    "            _, preds = torch.max(logits, 1)\n",
    "            \n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    # Вычисляем F1 Score\n",
    "    val_loss = total_loss / len(dataloader)\n",
    "    # print(f\"Val loss: {val_loss:.4f}\")\n",
    "    f1 = f1_score(all_labels, all_preds, average='micro')\n",
    "    # print(f\"F1 Score on test: {f1:.4f}\")\n",
    "    return f1, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            \n",
    "            batch_x, batch_y = batch_x.float(), batch_y.long()\n",
    "            \n",
    "            outputs = model(batch_x)\n",
    "            logits = F.softmax(outputs)\n",
    "            \n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        train_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
    "        \n",
    "        f1, val_loss = evaluate(model, test_loader)\n",
    "        print(f'Val loss: {val_loss}')\n",
    "        \n",
    "        if len(val_losses)==0 or val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f'MLP.pt')\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5102\n",
      "Val loss: 0.4696042835712433\n",
      "Epoch [2/20], Loss: 0.3519\n",
      "Val loss: 0.2064206600189209\n",
      "Epoch [3/20], Loss: 0.3377\n",
      "Val loss: 0.4392228424549103\n",
      "Epoch [4/20], Loss: 0.3277\n",
      "Val loss: 0.35340189933776855\n",
      "Epoch [5/20], Loss: 0.3187\n",
      "Val loss: 0.3160439729690552\n",
      "Epoch [6/20], Loss: 0.3106\n",
      "Val loss: 0.18352657556533813\n",
      "Epoch [7/20], Loss: 0.3039\n",
      "Val loss: 0.3154807388782501\n",
      "Epoch [8/20], Loss: 0.2953\n",
      "Val loss: 0.3168078064918518\n",
      "Epoch [9/20], Loss: 0.2870\n",
      "Val loss: 0.14822310209274292\n",
      "Epoch [10/20], Loss: 0.2798\n",
      "Val loss: 0.42775392532348633\n",
      "Epoch [11/20], Loss: 0.2734\n",
      "Val loss: 0.34568604826927185\n",
      "Epoch [12/20], Loss: 0.2658\n",
      "Val loss: 0.32977738976478577\n",
      "Epoch [13/20], Loss: 0.2584\n",
      "Val loss: 0.4559895992279053\n",
      "Epoch [14/20], Loss: 0.2516\n",
      "Val loss: 0.28696921467781067\n",
      "Epoch [15/20], Loss: 0.2457\n",
      "Val loss: 0.3889122009277344\n",
      "Epoch [16/20], Loss: 0.2388\n",
      "Val loss: 0.36525294184684753\n",
      "Epoch [17/20], Loss: 0.2314\n",
      "Val loss: 0.29930660128593445\n",
      "Epoch [18/20], Loss: 0.2264\n",
      "Val loss: 0.25882622599601746\n",
      "Epoch [19/20], Loss: 0.2174\n",
      "Val loss: 0.29272904992103577\n",
      "Epoch [20/20], Loss: 0.2116\n",
      "Val loss: 0.4805905520915985\n",
      "F1 score on test: 0.8804166666666666\n"
     ]
    }
   ],
   "source": [
    "input_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 4\n",
    "\n",
    "\n",
    "model = MLP(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RAdam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses, val_losses = train(model, train_loader, criterion, optimizer, num_epochs=num_epochs)\n",
    "f1, loss = evaluate(model, test_loader)\n",
    "print(f'F1 score on test: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.884625\n",
      "Loss: 0.16075220704078674\n"
     ]
    }
   ],
   "source": [
    "best_model = MLP(input_dim, hidden_dim, output_dim)\n",
    "best_model.load_state_dict(torch.load('MLP.pt'))\n",
    "\n",
    "f1, loss = evaluate(best_model, test_loader)\n",
    "print(f'F1 score on test: {f1}')\n",
    "print(f'Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = predict(best_model, test_loader_no_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.884625"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test_, test_preds, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(best_model, test_subm_loader)\n",
    "\n",
    "preds = np.array(preds) + 1\n",
    "ids = [i for i in range(len(preds))]\n",
    "\n",
    "subm_df = pd.DataFrame({'ID': ids, 'Class Index': preds})\n",
    "subm_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пробуем векторизировать и классифицировать с помощью LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_lstm(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch_x, batch_y in tqdm(dataloader):\n",
    "            \n",
    "            batch_x, batch_y = batch_x.long(), batch_y.long()\n",
    "            \n",
    "            outputs = model(batch_x)\n",
    "            logits = F.softmax(outputs)\n",
    "            \n",
    "            loss = criterion(outputs, batch_y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torcheval.metrics.functional import multiclass_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[329], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      3\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTM(hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocabulary_\u001b[49m), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "num_epoch = 5\n",
    "batch_size = 2\n",
    "hidden_dim = 100\n",
    "\n",
    "model = LSTM(hidden_dim=hidden_dim, vocab_size=len(vectorizer.vectorizer.vocabulary_), num_classes=4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_lstm(model, train_loader, criterion, optimizer, num_epochs=num_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
